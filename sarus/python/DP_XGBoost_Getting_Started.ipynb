{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started"
      ],
      "metadata": {
        "id": "eCJrIDxSECnM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mah07BlpD8yj",
        "outputId": "0cf5f771-15f1-4fc9-c03b-813abf5c079a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dp-xgboost\n",
            "  Downloading dp-xgboost-0.2.8.tar.gz (726 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m726.5/726.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dp-xgboost) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dp-xgboost) (1.11.4)\n",
            "Building wheels for collected packages: dp-xgboost\n",
            "  Building wheel for dp-xgboost (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dp-xgboost: filename=dp_xgboost-0.2.8-cp310-cp310-linux_x86_64.whl size=2834016 sha256=1f9ec9deb4dde3ec79fa11ab47a1e2b6843e423cb99253c62577725708c225eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/97/cc/f81e53a0485346573bc0ac30abfc872406b6d0fac35478bdd6\n",
            "Successfully built dp-xgboost\n",
            "Installing collected packages: dp-xgboost\n",
            "Successfully installed dp-xgboost-0.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install dp-xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification example"
      ],
      "metadata": {
        "id": "MVXkuXDSER3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dp_xgboost as xgb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification, load_svmlight_file\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, log_loss, accuracy_score\n",
        "import time\n",
        "\n",
        "dp_per_tree = 0.25\n",
        "n_trees = 20\n",
        "subsample = 0.2\n",
        "\n",
        "# we transform the classification problem into a regression one\n",
        "obj = 'reg:squarederror'\n",
        "\n",
        "x, y = make_classification(n_samples = 100000, n_features = 50, random_state = 100)\n",
        "\n",
        "base_score = 0.5\n",
        "scorefunc = mean_squared_error\n",
        "\n",
        "total_budget_spent = n_trees * np.log(1 + subsample*(np.exp(dp_per_tree) - 1))\n",
        "print('Total epsilon spent ', total_budget_spent)\n",
        "\n",
        "trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)\n",
        "\n",
        "n_data = trainX.shape[0]\n",
        "n_features = trainX.shape[1]\n",
        "\n",
        "feature_min = []\n",
        "feature_max = []\n",
        "\n",
        "# we need the feature bounds to build the DMatrix for DP training\n",
        "for i in range(n_features):\n",
        "    feature_min.append( min(trainX[:,i]) )\n",
        "for i in range(n_features):\n",
        "    feature_max.append( max(trainX[:,i]) )\n",
        "\n",
        "dtrain = xgb.DMatrix(trainX, label=trainY, feature_min=feature_min,\n",
        "    feature_max=feature_max)\n",
        "\n",
        "dtest = xgb.DMatrix(testX, label=testY, feature_min=feature_min,\n",
        "    feature_max=feature_max)\n",
        "\n",
        "print('DMatrix built')\n",
        "\n",
        "paramsDP =  {'objective': obj,\n",
        "        'tree_method':'approxDP', # this is Sarus XGBoost tree updater\n",
        "        'dp_epsilon_per_tree': dp_per_tree,\n",
        "        'max_depth': 6,\n",
        "        #'verbosity' : 3,\n",
        "        'learning_rate' : 0.2,\n",
        "        'lambda' : 0.1,\n",
        "        'base_score' : base_score,\n",
        "        'subsample' : subsample,\n",
        "        'min_child_weight' : 500,\n",
        "        'nthread' : 4}\n",
        "\n",
        "paramsNonDP =  {'objective': obj,\n",
        "        'tree_method':'approx',\n",
        "        'max_depth': 6,\n",
        "        'learning_rate' : 0.7,\n",
        "        'lambda' : 0.1,\n",
        "        'base_score' : base_score,\n",
        "        'subsample' : subsample,\n",
        "        'min_child_weight' : 100,\n",
        "        'nthread' : 4}\n",
        "\n",
        "begin = time.time()\n",
        "bstDP = xgb.train(paramsDP, dtrain, num_boost_round=n_trees)\n",
        "end = time.time()\n",
        "\n",
        "runtime_dp = end - begin\n",
        "\n",
        "begin = time.time()\n",
        "bst = xgb.train(paramsNonDP, dtrain, num_boost_round=n_trees)\n",
        "end = time.time()\n",
        "\n",
        "runtime_non_dp = end - begin\n",
        "\n",
        "predDP = bstDP.predict(dtest)\n",
        "predNonDP = bst.predict(dtest)\n",
        "\n",
        "test_errors_dp =  100 * (1-accuracy_score(testY, predDP > 0.5))\n",
        "test_errors_non_dp =  100 * (1-accuracy_score(testY, predNonDP > 0.5))\n",
        "\n",
        "print('test error DP', test_errors_dp)\n",
        "print('test error non-DP', test_errors_non_dp)\n",
        "print('runtime DP', runtime_dp)\n",
        "print('runtime non DP', runtime_non_dp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1oSiTm1EQdZ",
        "outputId": "d8e4eddc-73b2-4a6d-aebd-b12407637cee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dp_xgboost/compat.py:32: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
            "  from pandas import MultiIndex, Int64Index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total epsilon spent  1.105005686739905\n",
            "DMatrix built\n",
            "test error DP 14.934999999999999\n",
            "test error non-DP 5.4350000000000005\n",
            "runtime DP 4.209083557128906\n",
            "runtime non DP 3.5189671516418457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression example"
      ],
      "metadata": {
        "id": "XIHRKAYKGgx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dp_xgboost as xgb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, log_loss, accuracy_score\n",
        "import time\n",
        "\n",
        "dp_per_tree = 1\n",
        "n_trees = 20\n",
        "subsample = 0.2\n",
        "\n",
        "obj = 'reg:squarederror'\n",
        "\n",
        "x, y = make_regression(n_samples = 100000, n_features = 50, random_state = 100)\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "scaler.fit(y.reshape(-1,1))\n",
        "y = scaler.transform(y.reshape(-1,1))\n",
        "multiplier = (scaler.data_max_ - scaler.data_min_) / 2\n",
        "\n",
        "scorefunc = mean_squared_error\n",
        "base_score = 0\n",
        "\n",
        "total_budget_spent = n_trees * np.log(1 + subsample*(np.exp(dp_per_tree) - 1))\n",
        "print('Total epsilon spent ', total_budget_spent)\n",
        "\n",
        "trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)\n",
        "\n",
        "n_data = trainX.shape[0]\n",
        "n_features = trainX.shape[1]\n",
        "\n",
        "feature_min = []\n",
        "feature_max = []\n",
        "# we need the feature bounds to build the DMatrix for DP training\n",
        "for i in range(n_features):\n",
        "    feature_min.append( min(trainX[:,i]) )\n",
        "for i in range(n_features):\n",
        "    feature_max.append( max(trainX[:,i]) )\n",
        "\n",
        "dtrain = xgb.DMatrix(trainX, label=trainY, feature_min=feature_min,\n",
        "    feature_max=feature_max)\n",
        "\n",
        "dtest = xgb.DMatrix(testX, label=testY, feature_min=feature_min,\n",
        "    feature_max=feature_max)\n",
        "\n",
        "print('DMatrix built')\n",
        "\n",
        "paramsDP =  {'objective': obj,\n",
        "        'tree_method':'approxDP', # this is Sarus XGBoost tree updater\n",
        "        'dp_epsilon_per_tree': dp_per_tree,\n",
        "        'max_depth': 6,\n",
        "        #'verbosity' : 3,\n",
        "        'learning_rate' : 0.3,\n",
        "        'lambda' : 0.1,\n",
        "        'base_score' : base_score,\n",
        "        'subsample' : subsample,\n",
        "        'min_child_weight' : 1000,\n",
        "        'nthread' : 4}\n",
        "\n",
        "paramsNonDP =  {'objective': obj,\n",
        "        'tree_method':'approx',\n",
        "        'max_depth': 6,\n",
        "        'learning_rate' : 0.3,\n",
        "        'lambda' : 0.1,\n",
        "        'base_score' : base_score,\n",
        "        'subsample' : subsample,\n",
        "        'min_child_weight' : 2,\n",
        "        'nthread' : 4}\n",
        "\n",
        "\n",
        "begin = time.time()\n",
        "bstDP = xgb.train(paramsDP, dtrain, num_boost_round=n_trees)\n",
        "end = time.time()\n",
        "\n",
        "runtime_dp = end - begin\n",
        "\n",
        "begin = time.time()\n",
        "bst = xgb.train(paramsNonDP, dtrain, num_boost_round=n_trees)\n",
        "end = time.time()\n",
        "\n",
        "runtime_non_dp = end - begin\n",
        "\n",
        "predDP = bstDP.predict(dtest)\n",
        "predNonDP = bst.predict(dtest)\n",
        "\n",
        "test_errors_dp = multiplier * mean_squared_error(testY, predDP, squared=False)\n",
        "test_errors_non_dp = multiplier * mean_squared_error(testY, predNonDP, squared=False)\n",
        "\n",
        "print('test error DP', test_errors_dp)\n",
        "print('test error non-DP', test_errors_non_dp)\n",
        "print('runtime DP', runtime_dp)\n",
        "print('runtime non DP', runtime_non_dp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ4nj1y_GSqC",
        "outputId": "43f22b7b-fd55-414f-b710-3a2dd03a32ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total epsilon spent  5.907890582406953\n",
            "DMatrix built\n",
            "test error DP [149.24545138]\n",
            "test error non-DP [41.60890185]\n",
            "runtime DP 3.947401762008667\n",
            "runtime non DP 4.0069029331207275\n"
          ]
        }
      ]
    }
  ]
}